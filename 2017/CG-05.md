![WebAssembly logo](/images/WebAssembly.png)

## Agenda for the May meeting of WebAssembly's Community Group

- **Host**: Google, Mountain View, CA
- **Dates**: Tuesday-Wednesday May 30-31, 2017
- **Times**:
    - Tuesday - 9:00am to 5:00pm
    - Wednesday - 9:00am to 5:00pm
- **Location**: 1881 Landings Dr, Mountain View, CA 94043
- **Wifi**: GoogleGuest (no password)
- **Dinner**:
   Tenative: [Stein's Beer Garden](http://steinsbeergarden.com/)
   Wed May 31st. 5:30pm.
   Please sign up for dinner [here](https://goo.gl/FCh9eP).

- **Contact**:
    - Name: Brad Nelson
    - Phone: +1 650-214-2933
    - Email: bradnelson@google.com

### Registration

Please register before the event [here](https://goo.gl/forms/LQVhuRuI22lK8Umw1).

## Logistics

* Where to park
  - Free parking is available outside the building.

* How to access the building
  - The morning of event we'll meet you at the door.
  - At other times, please call or email the host.

* Technical presentation requirements (adapters, google hangouts/other accounts required, etc.)
  - Presentations will be done with a Google Hangout.
  - Contact host if you need alternatives.

* Any other logistics required to participate in the meeting
  - Please register before the event
    [here](https://goo.gl/forms/LQVhuRuI22lK8Umw1)
    as we may not be able to accommodate late arrivals.

### Hotels

[Nearby Hotels](https://www.google.com/maps/search/Hotels/@37.4199107,-122.0963779,15z/data=!3m1!4b1!4m8!2m7!3m6!1sHotels!2sMTV-LMK2,+1881+Landings+Dr,+Mountain+View,+CA+94043!3s0x808fba03c5d4f8d3:0xeeec4f675931b927!4m2!1d-122.0876231!2d37.4199111)

## Agenda items

1. Opening, welcome and roll call
    1. Opening of the meeting
    1. Introduction of attendees
    1. Host facilities, local logistics
1. Find volunteers for note taking
1. Adoption of the agenda
1. Proposals and discussions
    1. Working Process
    1. Threads (Ben Smith)
       * [Proposal](https://github.com/WebAssembly/threads/blob/master/proposals/threads/Overview.md)
       * Brief overview of the proposal
       * Topics for committee feedback
         - [i8 and i16 Value Types](https://github.com/WebAssembly/design/issues/1049)
         - [Encoding Scheme](https://github.com/WebAssembly/threads/issues/12)
         - [Matching SharedArrayBuffer/Atomics for v1](https://github.com/WebAssembly/threads/issues/5)
         - [Float atomics](https://github.com/WebAssembly/threads/issues/7)
         - [i64 atomics](https://github.com/WebAssembly/threads/issues/7)
         - [i64.wait](https://github.com/WebAssembly/threads/issues/7)
         - [Is the set of proposed instructions sufficient?](https://github.com/WebAssembly/threads/issues/11)
         - [Blocking main thread in web embedding](https://github.com/WebAssembly/threads/issues/6)
         - [Thread-local and Shared Global variables](https://github.com/WebAssembly/threads/issues/4)
         - [Consider supporting verifiably safe concurrency](https://github.com/WebAssembly/threads/issues/3)
         - [Native WebAssembly threads - needed for v1?](https://github.com/WebAssembly/threads/issues/8)
         - [Memory model standardization](https://github.com/WebAssembly/threads/issues/9)
         - [Memory orderings - sequential consistency only?](https://github.com/WebAssembly/threads/issues/13)
         - [How to test?](https://github.com/WebAssembly/threads/issues/10)
    1. Non-trapping float-to-int conversions [[1](https://github.com/WebAssembly/design/issues/986),[2](https://github.com/WebAssembly/meetings/pull/3#issuecomment-302154544)] (Dan Gohman) : 30 minutes
    1. SIMD (Jakob Stoklund Olesen)
       * [Proposal](https://github.com/WebAssembly/simd/blob/master/README.md)
       * Brief overview of the proposal
       * Topics for committee feedback
         - [SIMD types and basic operations](#simd-types-and-basic-operations)
         - [SIMD binary encoding](#simd-binary-encoding)
         - [Allow flushing of subnormals in floating point SIMD operations](#allow-flushing-of-subnormals-in-floating-point-simd-operations)
         - [Reciprocal (sqrt) approximation instructions](#reciprocal-sqrt-approximation-instructions)
         - [Alternative to Swizzle / Shuffle](#alternative-to-swizzle--shuffle)
       * Definition of performance benchmarks and acceptance criteria
    1. Working Group Formation
        * [Draft charter](https://webassembly.github.io/wg-charter/webassembly-wg-charter.html)
1. Closure

### Schedule constraints

## Dates and locations of future meetings

| Dates                    | Location          | Host       |
|--------------------------|-------------------|------------|
| 2017-07-17 to 2017-07-19 | San Francisco, CA | Google     |
| 2017-11-06 to 2017-11-07 | Burlingame, CA    | TPAC       |

## Proposal details

### SIMD proposal details

#### SIMD types and basic operations

Tracking issue: [Initial Proposal #1](https://github.com/WebAssembly/simd/pull/1).

The SIMD proposal uses a single `v128` value type to represent a 128-bit SIMD
vector.

**Poll**:

> Adopt a `v128` value type along with basic operations `v128.const`,
> `v128.load`, `v128.store` as well as bitwise logic instructions `v128.and`,
> `v128.or`, `v128.xor`, and `v128.not`.

Predicate vectors are represented with separate value types that don't have an
in-memory representation. This allows implementations to choose their own
representation. The `v*.select` operations are the primary consumers of
predicate vectors.

**Poll**:

> Adopt the `b8x16`, `b16x8`, `b32x4`, and `b64x2` value types along with the
> basic operations `b*.const`, `b*.splat`, `b*.extract_lane`,
> `b*.replace_lane`, as well as the logical operations `b*.and`,
> `b*.or`, `b*.xor`, `b*.not`, and `v*.select`.

Floating-point arithmetic is supported by the `f32x4.*` and `f64x2.*`
operations. The inclusion of `div` and `sqrt` will be discussed later, as will
the handling of subnormal numbers.

ARMv7 NEON supports `f32x4` arithmetic with subnormal numbers flushed to zero
only, and `f64x2` operations must be implemented with pairs of scalar VFP
instructions.

**Poll**:

> Adopt the following `f32x4.*` operations:
>
> - Basics: `f32x4.{splat,extract_lane,replace_lane}`.
> - Comparisons: `f32x4.{eq,ne,lt,le,gt,ge}`.
> - Arithmetic: `f32x4.{neg,abs,min,max,add,sub,mul}`.

**Poll**:

> Adopt the following `f64x2.*` operations:
>
> - Basics: `f64x2.{splat,extract_lane,replace_lane}`.
> - Comparisons: `f64x2.{eq,ne,lt,le,gt,ge}`.
> - Arithmetic: `f64x2.{neg,abs,min,max,add,sub,mul}`.

Small integer arithmetic is provided by the `i8x16.*`, `i16x8.*`, and
`i32x4.*` operations. These operations are widely supported by both Intel and
ARM ISAs. Integer division is not included since it is not widely supported.

**Poll**:

> Adopt the following `i8x16.*`, `i16x8.*`, and `i32x4.*` operations:
>
> - Basics: `{i8x16,i16x8,i32x4}.{splat,extract_lane*,replace_lane}`.
> - Arithmetic: `{i8x16,i16x8,i32x4}.{add,sub,mul,neg}`.
> - Shifts: `{i8x16,i16x8,i32x4}.{shl,shr_s,shr_u}`.
> - Equalities: `{i8x16,i16x8,i32x4}.{eq,ne}`.
> - Inequalities: `{i8x16,i16x8,i32x4}.{lt,le,gt,ge}_[su]`.

The `i64x2` operations are more limited, but they are still useful for bitwise
manipulation of `f64x2` vectors. Division and multiplication are not widely
available. Comparisons exist in SSE 4.2 and ARMv8 only.

**Poll**:

> Adopt the following `i64x2.*` operations:
>
> - Basics: `{i8x16,i16x8,i32x4}.{splat,extract_lane*,replace_lane}`.
> - Arithmetic: `{i8x16,i16x8,i32x4}.{add,sub,neg}`.
> - Shifts: `{i8x16,i16x8,i32x4}.{shl,shr_s,shr_u}`.

64-bit comparisons will be slow on ARMv7 and Intel chips without SSE 4.2.

**Poll**:

> Adopt the following `i64x2.` operations:
>
> - Equalities: `{i8x16,i16x8,i32x4}.{eq,ne}`.
> - Inequalities: `{i8x16,i16x8,i32x4}.{lt,le,gt,ge}_[su]`.

The 64-bit multiplication is implemented in AVX-512, but would be slower
everywhere else. All ISAs have 32 x 32 â€“> 64 multiplies (`pmuludq`, `umull`) that
can be used to implement this operation.

**Poll**:

> Adopt the `i64x2.mul` operation.

Saturating integer arithmetic is widely available for 8-bit and 16-bit lanes,
but not for `i32x4`.

**Poll**:

> Adopt the saturating integer arithmetic operations
> `{i8x16,i16x8}.{add,sub}_saturate_[su]`.

Integer-to-float conversions are always non-trapping.

**Poll**:

> Adopt the `f*.convert_[su]/i*` integer-to-float conversions.

When converting floating-point numbers to integers, we can choose to trap or
saturate on overflow.

**Poll**:

> Adopt the trapping `i*.trunc_[su]/f*` float-to-integer conversions.

**Poll**:

> Adopt the saturating `i*.trunc_[su]/f*:sat` float-to-integer conversions.


#### SIMD binary encoding

Tracking issue: [SIMD binary encoding #14](https://github.com/WebAssembly/simd/issues/14).

The current SIMD proposal defines 193 new operations, and it is likely that
more SIMD operations will be added in the future. We need an extensible way of
encoding the opcodes for these operations.

**Poll**:

> SIMD opcodes are encoded as `0xf1 varuint32`, where `0xf1` indicates a SIMD
> operation and the `varuint32` number identifies the specific operation.

This should be coordinated with the [threads proposal](https://github.com/WebAssembly/threads/blob/master/proposals/threads/Overview.md)
which currently uses a `0xf0` prefix byte followed by an opcode byte in the
range `0x00 - 0x7a`.

The new value types also need assigned numbers.

**Poll**:

> Assign the following numbers to the new SIMD value types:
>
> - `0x7b => v128`
> - `0x7a => b16x8`
> - `0x79 => b8x16`
> - `0x78 => b4x32`
> - `0x77 => b2x64`


#### Allow flushing of subnormals in floating point SIMD operations

Tracking issue: [Allow flushing of subnormals in floating point SIMD operations #2](https://github.com/WebAssembly/simd/issues/2).

ARMv7 NEON devices support `f32x4` arithmetic, but only with subnormals flushed
to zero. On these devices, `f64x2` arithmetic needs to be implemented with the
slower VFP instruction set which does support subnormals correctly, so the
problem only applies to `f32x4` operations.

**Poll**:

> Adopt the proposed specification text which allows SIMD floating-point
> instructions to flush subnormals to zero.


#### Reciprocal (sqrt) approximation instructions

Tracking issues: [Implementation-dependent reciprocal (sqrt) approximation instructions #3](https://github.com/WebAssembly/simd/issues/3) and [Eliminate divide, square root #13](https://github.com/WebAssembly/simd/issues/13).

Floating-point divide and square root instructions are slow. They take many
cycles to complete, and they are not fully pipelined, so they block the ALU
while they are executing. Further, ARMv7 NEON does not provide vectorized
floating-point divide and square root instructions, so they have to be
implemented as a sequence of scalar VFP instructions.

All SIMD ISAs provide fast `f32x4` approximation instructions that compute 9-12
bits of 1/x or 1/sqrt(x) respectively. These instructions are fully pipelined
and typically as fast as a floating-point addition. They don't provide the
exact same approximation on different platforms, though.

**Poll**:

> Adopt `f32x4` 1/x and 1/sqrt(x) approximation instructions.

The `f64x2` versions of the approximation instructions are only available in
AVX-512 and ARMv8's A64 mode.

**Poll**:

> Adopt `f64x2` 1/x and 1/sqrt(x) approximation instructions.

It has also been proposed to remove the exact division and square root
instructions since they are slow:

**Poll**:

> Remove the `f32x4.div` and `f64x2.div` instructions.

**Poll**:

> Remove the `f32x4.sqrt` and `f64x2.sqrt` instructions.

Presumably, the answer to these questions should depend on whether the
approximation instructions are adopted.

#### Alternative to Swizzle / Shuffle

Tracking issue: [Alternative to Swizzle / Shuffle #8](https://github.com/WebAssembly/simd/issues/8).

The initial SIMD proposal contains fully general swizzle and shuffle
instructions that take a sequence of lane indexes as immediate operands. An
alternative proposal is to only define a subset of the possible swizzle and
shuffle operations that are known to map to fast instructions.

The `v8x16.shuffle` instruction is pivotal since its functionality subsumes all
the other swizzle and shuffle instructions. It can be implemented in terms of
`pshufb` or `vtbl` instructions that are quite fast, see the GitHub issue for
details.

**Poll**:

> Adopt the `v8x16.shuffle` instruction.

The `v8x16.shuffle` instruction is quite large with its 16 bytes of immediate
operands. Most of the time, shuffles with larger granularity suffice, and they
can be encoded with smaller immediate operands (one byte per lane).

**Poll**:

> Adopt the `{v16x8,v32x4,v64x2}.shuffle` instructions.

The `v*.swizzle(x)` instructions pick lanes from a single input vector instead
of two. Their functionality is subsumed by `v*.shuffle(x, x)` which requires a
`get_local`.

**Poll**:

> Adopt the `v*.swizzle` instructions.

SIMD ISAs have various shuffle instructions that are faster than the fully
general `pshufb` and `vtbl` instructions. An implementation is free to use
these instructions for the corresponding immediate operands on `v8x16.shuffle`.
The alternative proposal in the tracking GitHub issue is to define a set of
fixed shuffle instructions that are known to map to these fast instructions on
all ISAs.

**Poll**:

> Define a small set of primitive permutations that we know can be implemented
> efficiently.


## Meeting notes

Notes added here after the meeting.
